\documentclass[12pt, a4paper]{article}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage{subfiles}
\usepackage{graphicx}
\usepackage[hyphens,spaces]{url}

\usepackage{abstract}
% \renewcommand{\abstractnamefont}{\normalfont\Large\bfseries}
% \renewcommand{\abstracttextfont}{\normalfont\Small}

\linespread{1.25}
\newtheorem{definition}{Definition}


\begin{document}
  \subfile{sections/titlepages}

  \begin{abstract}
    The phenomenon of fake news has been escalating alarmingly in the last few decades, since the outburst of the Internet and social media. The COVID-19 pandemic illustrates perfectly how dangerously and quickly lies, fabricated news, hoaxes and conspiracies can be disseminated at a global level, resulting into mass misinformation, revolt, paranoia and a steep division between people. Given that most people inform themselves about essentially everything through online sources, there is a great need for a collective endeavour to mitigate this issue. In this context, fake news detection has emerged as an increasingly researched field over the last years.
    
    This thesis examines several approaches for evaluating the reliability and deceitfulness of online news articles. Primarily, the methodology includes supervised machine learning algorithms, such as K-Nearest Neighbors, Decision Trees, Logistic Regression and Support Vector Machines, which are leveraged for classifying news articles by veracity, based upon their contents and headlines. The results were satisfactory, the best models achieving over 90\% accuracy scores on the datasets, while also performing promisingly well on real-life inputs. Secondarily, some non-AI methods were also implemented, by means of web-scraping and crowdsourcing All of these news analysis procedures were subsequently brought together under a browser extension, with the final goal of providing users with an accessible tool for evaluating the reliability of any news article.

    This work is the result of my own activity. I have neither given nor received unauthorized assistance on this work.
  \end{abstract}
  \newpage

  \tableofcontents
  \newpage

  \section{Introduction}
    Fake news is not simply a product of the last decades. It has been part of humanity for at least centuries. Its first significant outburst dates back to 1400s, when the printing press was invented. Ever since then, fabricated news have been more and more weaponized to manipulate people at increasingly larger scales. In 1835, The New York Sun published 6 articles presenting the discovery of life on the moon. Both World Wars were heavily marked by propaganda, and false and misleading news \cite{a4}. And more recently, the US 2016 presidential elections and the COVID-19 pandemic were subjects of large amounts of fake news.

    Despite being so widespread, there is no globally agreed definition of the "fake news" term. Nonetheless, many sources provide definitions which have two primary elements in common: authenticity and intent. As for authenticity, fake news contain claims that are factually and verifiably false. As for intent, the major purpose of fake news is to deceive the consumers \cite{a2}. That being said, a concise definition of fake news could be formulated in the following manner:

    \begin{definition}
      By fake news we mean any news item that is deliberately and factually untrue and misleading.
    \end{definition}

    Fake News Detection represents a new area of study. On the one hand, it emerged thanks to the continuous advancements in Artificial Intelligence and, specifically, machine learning supervised learning methods. On the other hand, it appeared as virtually a necessity, given the recent proliferation of the fake news phenomenon, on account of the simultaneous raise of digital media.

    Digital and social media are environments in which the dissemination of information is more facile and rapid than at any point in history. Fabricated news have been heavily published and shared as a means of deception, on subjects of high impact, such as the 2016 US presidential elections and the COVID-19 pandemic.
  
    At present, there are multiple fake news identification resources online. They each contribute in various manners to validating the veracity of news and debunking rumours, hoaxes, conspiracies \cite{a10}: PolitiFact and The Washington Post Fact Checker analyze the statements and claims of American politicians; Snopes, FullFact, HoaxSlayer and GossipCop debunk fake articles containing fake news, gossips, hoaxes or celebrity rumors.

    Although these online fact-checking resources have a notable and admirable contribution to combating fake news, the incredibly high rate at which fabricated information is produced and distributed through unreliable news agencies and suspicious social media accounts, makes the task of manual fake news detection impactful at only a very small scale. That is why, computer science, artificial intelligence and machine learning algorithms could have a more promising large-scale impact and efficiency. 
    
    Therefore, the aim of this thesis is to explore and implement a series of algorithms to aid users in protecting themselves against misinformation, by means of machine learning algorithms together with other computer science methods, such as web-scraping and crowdsourcing. As for the structure of the thesis, Chapter 2 provides a rundown of the machine learning notions and algorithms leveraged for the creation of some fake news detection content-based and headline-based classifiers. Chapter 3 reviews some fake news detection related research work, exploring several methodologies in which machine learning and deep learning can be utilized. Chapter 4 gives a comprehensive account of the whole application development process, including the decision-making process, the technologies employed, the architecture of the application, the machine learning algorithms used, how the news sources reliability was determined via web-scraping and how crowdsourcing / user feedback can also be utilized for building a reliability profile for news sources and authors, while also building a new dataset in parallel. Chapter 5 ends the thesis with some final concluding words and what wants to achieve as part of a future effort.
  \newpage

  \subfile{sections/ml-and-nlp}

  \section{Related Work}
    
  \newpage

  \subfile{sections/application-development}

  \newpage
  \section{Conclusion and Future Work}
  This thesis explored the proliferating phenomenon of fake news, delved into the novel area of research dedicated to combating and detecting fake news, and provided an overview on the application development process, from inception to completion, concentrating primarily on machine learning approaches and secondarily on other possible approaches, such as web-scraping and crowdsourcing. 
  
  The machine learning algorithms used for classifying article news as fake or real based upon their contents and headlines yielded good results, the best ones achieving over 90\% accuracy scores, while also performing promisingly well on real input data.
  
  The goal of this thesis to provide a straightforward, well-structured and easy-to-use tool whereby users can perform multi-faceted analyzes on the reliability of online news was met to satisfactory degree, even though I consider this project to be at its first version, with a great amount of additional functionalities that can be implemented and improvements that can be performed. 
  
  On the one hand, there is definitely place for lots of improvements to the machine learning algorithms. Even though they performed well from the standpoint of machine learning metrics, the datasets in the field of fake news detection are not yet at an entirely satisfactory stage. Most datasets have flaws, the most important including the limited amount of articles as well as the often disputed, subjective and questionable process through which news are labeled as fake or real. Given the disputable nature of the datasets, unsupervised learning methods could represent a promising alternative that should be explored. Moreover, deep learning algorithms such as LSTM (long short-term memory) could in all likelihood generate promising and perhaps better results as well.

  On the other hand, there are many functionalities ideas that did not get to be implemented in this first version of the application. First and foremost, given that social media networks, such as Twitter and Facebook, are the environments in which fake news are most rapidly disseminated, it would be a critical step to integrate this web extension to analyze the reliability of the social media posts. Secondly, automatically performing reliability analyzes and fake news detection on the Google Search results, would categorically enhance the experience of the user and would help them to select more easily the source from which to inform themselves. Lastly, many improvements and additions could be made to the crowdsourcing or user feedback side of the application, in order to curate the list of received feedbacks and collect a reliable and constantly updated dataset of fake and real news collected from past user feedbacks and machine learning predictions, that could be further on used for training new machine learning models.

  \newpage
  \bibliographystyle{unsrt}
  \bibliography{sources}
  \nocite{*}
\end{document}