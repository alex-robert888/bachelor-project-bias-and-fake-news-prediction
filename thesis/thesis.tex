\documentclass[12pt, a4paper]{article}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage{subfiles}
\usepackage{graphicx}
\usepackage[hyphens,spaces]{url}

\usepackage{abstract}
\usepackage{float}
% \renewcommand{\abstractnamefont}{\normalfont\Large\bfseries}
% \renewcommand{\abstracttextfont}{\normalfont\Small}

\linespread{1.25}
\newtheorem{definition}{Definition}


\begin{document}
  \subfile{sections/titlepages}

  \begin{abstract}
    The phenomenon of fake news has been escalating alarmingly in the last few decades, since the outburst of the Internet and social media. The COVID-19 pandemic illustrates perfectly how dangerously and quickly lies, fabricated news, hoaxes and conspiracies can be disseminated at a global level, resulting into mass misinformation, revolt, paranoia and a steep division between people. Given that most people inform themselves about essentially everything through online sources, there is a great need for a collective endeavour to mitigate this issue. In this context, fake news detection has emerged as an increasingly researched field over the last decades.
    
    This thesis examines several approaches for evaluating the reliability and deceitfulness of online news articles. Primarily, the methodology includes supervised machine learning algorithms, such as K-Nearest Neighbors, Decision Trees, Logistic Regression and Support Vector Machines, which are leveraged for classifying news articles by veracity, based upon their contents and headlines. The results were satisfactory, the best models achieving over 90\% accuracy scores on the datasets, while also performing promisingly well on real-life inputs. Secondarily, some non-AI methods were also implemented, by means of web-scraping and crowdsourcing All of these news analysis procedures were subsequently brought together under a browser extension, with the final goal of providing users with an accessible tool for evaluating the reliability of any news article.

    This work is the result of my own activity. I have neither given nor received unauthorized assistance on this work.
  \end{abstract}
  \newpage

  \tableofcontents
  \newpage

  \section{Introduction}
    Fake news is not simply a product of the last decades. It has been part of humanity for at least centuries. Its first significant outburst dates back to 1400s, when the printing press was invented. Ever since then, fabricated news have been more and more weaponized to manipulate people at increasingly larger scales. In 1835, The New York Sun published 6 articles presenting the discovery of life on the moon. Both World Wars were heavily marked by propaganda, and false and misleading news \cite{a4}. And more recently, the US 2016 presidential elections and the COVID-19 pandemic were subjects of large amounts of fake news.

    Despite being so widespread, there is no globally agreed definition of the "fake news" term. Nonetheless, many sources provide definitions which have two primary elements in common: authenticity and intent. As for authenticity, fake news contain claims that are factually and verifiably false. As for intent, the major purpose of fake news is to deceive the consumers \cite{a2}. That being said, a concise definition of fake news could be formulated in the following manner:

    \begin{definition}
      By fake news we mean any news item that is deliberately and factually untrue and misleading.
    \end{definition}

    Fake News Detection represents a new area of study. On the one hand, it emerged thanks to the continuous advancements in Artificial Intelligence and, specifically, machine learning supervised learning methods. On the other hand, it appeared as virtually a necessity, given the recent proliferation of the fake news phenomenon, on account of the simultaneous raise of digital media.

    Digital and social media are environments in which the dissemination of information is more facile and rapid than at any point in history. Fabricated news have been heavily published and shared as a means of deception, on subjects of high impact, such as the 2016 US presidential elections and the COVID-19 pandemic.
  
    At present, there are multiple fake news identification resources online. They each contribute in various manners to validating the veracity of news and debunking rumours, hoaxes, conspiracies \cite{a10}: PolitiFact and The Washington Post Fact Checker analyze the statements and claims of American politicians; Snopes, FullFact, HoaxSlayer and GossipCop debunk fake articles containing fake news, gossips, hoaxes or celebrity rumors.

    Although these online fact-checking resources have a notable and admirable contribution to combating fake news, the incredibly high rate at which fabricated information is produced and distributed through unreliable news agencies and suspicious social media accounts, makes the task of manual fake news detection impactful at only a very small scale. That is why, computer science, artificial intelligence and machine learning algorithms could have a more promising large-scale impact and efficiency. 
    
    Therefore, the aim of this thesis is to explore and implement a series of algorithms to aid users in protecting themselves against misinformation, by means of machine learning algorithms together with other computer science methods, such as web-scraping and crowdsourcing. As for the structure of the thesis, Chapter 2 provides a rundown of the machine learning notions and algorithms leveraged for the creation of some fake news detection content-based and headline-based classifiers. Chapter 3 reviews some fake news detection related research work, exploring several methodologies in which machine learning and deep learning can be utilized. Chapter 4 gives a comprehensive account of the whole application development process, including the decision-making process, the technologies employed, the architecture of the application, the machine learning algorithms used, how the news sources reliability was determined via web-scraping and how crowdsourcing / user feedback can also be utilized for building a reliability profile for news sources and authors, while also building a new dataset in parallel. Chapter 5 ends the thesis with some final concluding words and what wants to achieve as part of a future effort.
  \newpage

  \subfile{sections/ml-and-nlp}

  \newpage
  \section{Fake News Detection Methods Overview}
    The last decades marked important advancements in the field of fake news detection. These advancements coincide non only with the exponential rise of fake news owed to the communication and information breakthroughs brought about by the Internet and social media, but also with the remarkable progress recorded in computer science along with artificial intelligence and its subfield of machine learning.

    In the relatively novel literature of fake news detection, the solid prospects of machine learning algorithms are very frequently discussed. There are several manners in which machine learning can be leveraged for the identification of fake news, but most of them revolve around two main categories: news content-based and social context-based fake news detection \cite{a2}. In what follows in this chapter, these two different types of methods will be further detailed.

    On the one hand, news content-based fake news detection incorporates the machine learning approaches which predict the factuality and deceitfulness of news by analyzing the metadata of news article, consisting of the source, headline, body text and images/videos.

    B. Horne and S. Adali et. al \cite{a13} point out that there is often an underlying presumption that the editorial style in which fake news are written are made to be as similar as possible to real news. Nonetheless, this assumption is most of the times inaccurate and as a matter of fact, the body content of the news can be highly evocative in terms of the reliability of the facts presented in the news. An important element making fake news stand out is the persuasion element, which is most of the times achieved through heuristics rather than strong arguments.

    In addition, B. Horne and S. Adali, as well as Y. Chen and J. Conroy et. al \cite{a14} highlight that both the body content as well as the headline are very often characterized by clickbait, which refers to the opinionated and inflammatory language used in order to entice reader to read the full article. Therefore, before even reading the content of the article, the lingustic features of tge headline itself can be exploited straight away to predict the misleading intent of the article, often crafted for political or financial gain.

    Moreover, extracting other linguistic feature from the body of the content, such as number of quotes, the average length of the quotes, number of external links, number of sources cited, number and length of the graphs can also predict how well fact-checked and credible are the information presented \cite{a15}.
    
    Besides the headline and the content of the news articles, the visual cues have been proven to be a critical element of propagandistic pieces of news. Images can inspire emotions and legitimacy to the stories, in spite of not bringing little to no arguments or evidence \cite{visual_cues}.

    On the other hand, in addition to the content-based algorithms utilizing the metadata of the news articles, there is also some research considering the detection based upon the social context to be even more effective.

    There is some literature exploring user-based approaches, working on the observations that most fake news are spread by non-human social media accounts or also called, bots. Individual-level features are exploited to deduce the overall credibility of the user, by inspecting various demographics of the suspicious accounts, such as registration age, number of followers/followees, number of tweets the user has posted / authored and so forth \cite{a16}. Apart from individual-level features, some group-level approaches can also capture some important characteristics of the environment in which fake news are spread. Group-level properties are based upon the observation that both fake and real news spreaders have the tendency of creating and adhering to social media communities with the same purpose. Consequently, fake news can be detected by following some characteristics of these groups, such as  percentage of verified users, average number of followers or various other statistics that are build by computing the average of individual-level feature of the users comprising the groups.  
  
  \newpage

  \subfile{sections/application-development}

  \newpage
  \section{Conclusion and Future Work}
  This thesis explored the proliferating phenomenon of fake news, delved into the novel area of research dedicated to combating and detecting fake news, and provided an overview on the application development process, from inception to completion, concentrating primarily on machine learning approaches and secondarily on other possible approaches, such as web-scraping and crowdsourcing. 
  
  The machine learning algorithms used for classifying article news as fake or real based upon their contents and headlines yielded good results, the best ones achieving over 90\% accuracy scores, while also performing promisingly well on real input data.
  
  The goal of this thesis to provide a straightforward, well-structured and easy-to-use tool whereby users can perform multi-faceted analyzes on the reliability of online news was met to satisfactory degree, even though I consider this project to be at its first version, with a great amount of additional functionalities that can be implemented and improvements that can be performed. 
  
  On the one hand, there is definitely place for lots of improvements to the machine learning algorithms. Even though they performed well from the standpoint of machine learning metrics, the datasets in the field of fake news detection are not yet at an entirely satisfactory stage. Most datasets have flaws, the most important including the limited amount of articles as well as the often disputed, subjective and questionable process through which news are labeled as fake or real. Given the disputable nature of the datasets, unsupervised learning methods could represent a promising alternative that should be explored. Moreover, deep learning algorithms such as LSTM (long short-term memory) could in all likelihood generate promising and perhaps better results as well.

  On the other hand, there are many functionalities ideas that did not get to be implemented in this first version of the application. First and foremost, given that social media networks, such as Twitter and Facebook, are the environments in which fake news are most rapidly disseminated, it would be a critical step to integrate this web extension to analyze the reliability of the social media posts. Secondly, automatically performing reliability analyzes and fake news detection on the Google Search results, would categorically enhance the experience of the user and would help them to select more easily the source from which to inform themselves. Lastly, many improvements and additions could be made to the crowdsourcing or user feedback side of the application, in order to curate the list of received feedbacks and collect a reliable and constantly updated dataset of fake and real news collected from past user feedbacks and machine learning predictions, that could be further on used for training new machine learning models.

  \newpage
  \bibliographystyle{unsrt}
  \bibliography{sources}
  \nocite{*}
\end{document}